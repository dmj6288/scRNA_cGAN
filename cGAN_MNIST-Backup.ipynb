{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bd3c19-69f5-4eb3-8e01-e3728881b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=10, image_channel=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.gen = nn.Sequential(\n",
    "            self._generator_block(input_dim, hidden_dim * 4),\n",
    "            self._generator_block(\n",
    "                hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1\n",
    "            ),\n",
    "            self._generator_block(hidden_dim * 2, hidden_dim),\n",
    "            self._generator_block(\n",
    "                hidden_dim, image_channel, kernel_size=4, final_layer=True\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _generator_block(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        kernel_size=3,\n",
    "        stride=2,\n",
    "        final_layer=False,\n",
    "    ):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    input_channels, output_channels, kernel_size, stride\n",
    "                ),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    input_channels, output_channels, kernel_size, stride\n",
    "                ),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "def create_noise_vector(n_samples, input_dim, device=\"cpu\"):\n",
    "    return torch.randn(n_samples, input_dim, device=device)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_channel=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self._discriminator_block(image_channel, hidden_dim),\n",
    "            self._discriminator_block(hidden_dim, hidden_dim * 2),\n",
    "            self._discriminator_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def _discriminator_block(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        final_layer=False,\n",
    "    ):\n",
    "\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09713618-c236-49de-880b-e8d2c77bee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)  # Set for our testing purposes, please do not change!\n",
    "\n",
    "\n",
    "def plot_images_from_tensor(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    \"\"\"\n",
    "    Plots a grid of images from a given tensor.\n",
    "\n",
    "    The function first scales the image tensor to the range [0, 1]. It then detaches the tensor from the computation\n",
    "    graph and moves it to the CPU if it's not already there. After that, it creates a grid of images and plots the grid.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A 4D tensor containing the images.\n",
    "            The tensor is expected to be in the shape (batch_size, channels, height, width).\n",
    "        num_images (int, optional): The number of images to include in the grid. Default is 25.\n",
    "        size (tuple, optional): The size of a single image in the form of (channels, height, width). Default is (1, 28, 28).\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid. The final grid size is (num_images // nrow, nrow). Default is 5.\n",
    "        show (bool, optional): Determines if the plot should be shown. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        None. The function outputs a plot of a grid of images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the image tensor to [0, 1]\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "\n",
    "    # Detach the tensor from its computation graph and move it to the CPU\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "\n",
    "    # Create a grid of images using the make_grid function from torchvision.utils\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "\n",
    "    # Plot the grid of images\n",
    "    # The permute() function is used to rearrange the dimensions of the grid for plotting\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "\n",
    "    # Show the plot if the 'show' parameter is True\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\"\"\" The reason for doing \"image_grid.permute(1, 2, 0)\"\n",
    "\n",
    "PyTorch modules processing image data expect tensors in the format C × H × W.\n",
    "\n",
    "Whereas PILLow and Matplotlib expect image arrays in the format H × W × C\n",
    "\n",
    "so to use them with matplotlib you need to reshape it\n",
    "to put the channels as the last dimension:\n",
    "\n",
    "I could have used permute() method as well like below\n",
    "\"np.transpose(npimg, (1, 2, 0))\"\n",
    "\n",
    "------------------\n",
    "\n",
    "Tensor.detach() is used to detach a tensor from the current computational graph. It returns a new tensor that doesn't require a gradient.\n",
    "\n",
    "When we don't need a tensor to be traced for the gradient computation, we detach the tensor from the current computational graph.\n",
    "\n",
    "We also need to detach a tensor when we need to move the tensor from GPU to CPU.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize the weights of convolutional and batch normalization layers.\n",
    "\n",
    "    Args:\n",
    "        m (torch.nn.Module): Module instance.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def ohe_vector_from_labels(labels, n_classes):\n",
    "    return F.one_hot(labels, num_classes=n_classes)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.tensor([4, 3, 2, 1, 0])\n",
    "F.one_hot(x, num_classes=6)\n",
    "\n",
    "# Expected result\n",
    "# tensor([[0, 0, 0, 0, 1, 0],\n",
    "#         [0, 0, 0, 1, 0, 0],\n",
    "#         [0, 0, 1, 0, 0, 0],\n",
    "#         [0, 1, 0, 0, 0, 0],\n",
    "#         [1, 0, 0, 0, 0, 0]])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "1. All tensors need to have the same number of dimensions and\n",
    "2. All dimensions except the one that they are concatenated on, need to have the same size. \"\"\"\n",
    "\n",
    "\n",
    "def concat_vectors(x, y):\n",
    "    \"\"\"\n",
    "    Concatenate two tensors along the second dimension.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): First input tensor.\n",
    "        y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Concatenated tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    combined = torch.cat((x.float(), y.float()), 1)\n",
    "    return combined\n",
    "\n",
    "def calculate_input_dim(z_dim, mnist_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate the input dimensions for the generator and discriminator networks.\n",
    "\n",
    "    Args:\n",
    "        z_dim (int): Dimension of the random noise vector (latent space).\n",
    "        mnist_shape (tuple): Shape of the MNIST images, e.g., (1, 28, 28).\n",
    "        n_classes (int): Number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the generator input dimension and discriminator image channel.\n",
    "\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\"\"\"\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "\n",
    "    # mnist_shape[0] is 1 as its grayscale images\n",
    "    discriminator_image_channel = mnist_shape[0] + n_classes\n",
    "\n",
    "    return generator_input_dim, discriminator_image_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a803ab7-968c-4d7e-a808-fc8e35503cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from utils import *\n",
    "\n",
    "####################################################\n",
    "def test_weights_init():\n",
    "    # Create a sample model with Conv2d and BatchNorm2d layers\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ConvTranspose2d(16, 3, kernel_size=3),\n",
    "        nn.BatchNorm2d(3)\n",
    "    )\n",
    "\n",
    "    # Initialize the model weights\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # Check the weights of Conv2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "\n",
    "    # Check the weights of BatchNorm2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "            assert torch.allclose(module.bias, torch.tensor(0.0))\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_weights_init()\n",
    "\n",
    "####################################################\n",
    "def test_concat_vectors():\n",
    "    # Create sample input tensors\n",
    "    x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    y = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "    # Perform concatenation\n",
    "    combined = concat_vectors(x, y)\n",
    "\n",
    "    # Check the output type and shape\n",
    "    assert isinstance(combined, torch.Tensor)\n",
    "    assert combined.shape == (2, 6)  # Expected shape after concatenation\n",
    "\n",
    "    # Check the values in the concatenated tensor\n",
    "    expected_combined = torch.tensor([[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]])\n",
    "    assert torch.allclose(combined, expected_combined)\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_concat_vectors()\n",
    "\n",
    "####################################################\n",
    "def test_calculate_input_dim():\n",
    "    # Set up sample inputs\n",
    "    z_dim = 100\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\n",
    "\n",
    "    # Calculate input dimensions\n",
    "    generator_input_dim, discriminator_image_channel = calculate_input_dim(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "    # Check the output types and values\n",
    "    assert isinstance(generator_input_dim, int)\n",
    "    assert generator_input_dim == z_dim + n_classes\n",
    "\n",
    "    assert isinstance(discriminator_image_channel, int)\n",
    "    assert discriminator_image_channel == mnist_shape[0] + n_classes\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_calculate_input_dim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e628c79-5284-472c-8d98-94ab32e6fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from conditional_gan import *\n",
    "#from utils import *\n",
    "\n",
    "\n",
    "mnist_shape = (1, 28, 28)\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 64\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = \"cuda\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec866d11-cae5-44f2-bf62-a0dab855e6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e401bfb90a849e3b71ec265123643f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "Let Long Training Continue\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m gen_opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Keep track of the generator losses\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m generator_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mgen_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m##################################\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m#  Log Progress and Visualization\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m#  for each display_step = 50\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m##################################\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_step \u001b[38;5;241m%\u001b[39m display_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cur_step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# Calculate Generator Mean loss for the latest display_steps (i.e. latest 50 steps)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# list[-x:]   # last x items in the array\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST(\n",
    "        \"/home/dennis00/scRNA_GAN/MNIST\", download=True, transform=transform\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "generator_input_dim, discriminator_image_channel = calculate_input_dim(\n",
    "    z_dim, mnist_shape, n_classes\n",
    ")\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "disc = Discriminator(image_channel=discriminator_image_channel).to(device)\n",
    "\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "gen = gen.apply(weights_init)\n",
    "\n",
    "disc = disc.apply(weights_init)\n",
    "\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        # create one hot encoded vectors from labels and n_classes\n",
    "        one_hot_labels = ohe_vector_from_labels(labels.to(device), n_classes)\n",
    "        print(\"one_hot_labels \", one_hot_labels.size())  # => torch.Size([128, 10])\n",
    "\n",
    "        \"\"\" The above ([128, 10]) need to be converted to ([128, 10, 28, 28])\n",
    "\n",
    "        Because, Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "         1. All tensors need to have the same number of dimensions and\n",
    "         2. All dimensions except the one that they are concatenated on, need to have the same size.\n",
    "\n",
    "        To do that, first I am adding extra dimension with 'None'\n",
    "        the easiest way to add extra dimensions to an array is by using the keyword None,\n",
    "        when indexing at the position to add the extra dimension.\n",
    "        Note, in below with keyword None, I am only adding extra dummy empty dimension\n",
    "\n",
    "        a = torch.rand(1, 2)\n",
    "        ic(a) # => tensor([[0.1749, 0.6387]])\n",
    "        ic(a[None, :]) # => tensor([[[0.1749, 0.6387]]])\n",
    "\n",
    "        a = torch.rand([1,2,3,4])\n",
    "        ic(a.shape) # => torch.Size([1, 2, 3, 4])\n",
    "        ic(a[None, :].shape) # => torch.Size([1, 1, 2, 3, 4])\n",
    "        \"\"\"\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 1, 1])\n",
    "\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "            1, 1, mnist_shape[1], mnist_shape[2]\n",
    "        )\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 28, 28])\n",
    "\n",
    "        #########################\n",
    "        #  Train Discriminator\n",
    "        #########################\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size\n",
    "        fake_noise = create_noise_vector(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "        # Now we can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "\n",
    "        noise_and_labels = concat_vectors(fake_noise, one_hot_labels)\n",
    "        fake = gen(noise_and_labels)\n",
    "\n",
    "        # Make sure that enough images were generated\n",
    "        assert len(fake) == len(real)\n",
    "\n",
    "        # Now we can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels,\n",
    "        #              remember to detach the generator (.detach()) so we do not backpropagate\n",
    "        #              through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "\n",
    "        # Combine the fake images with image_one_hot_labels\n",
    "        fake_image_and_labels = concat_vectors(fake, image_one_hot_labels)\n",
    "\n",
    "        # Combine the real images with image_one_hot_labels\n",
    "        real_image_and_labels = concat_vectors(real, image_one_hot_labels)\n",
    "\n",
    "        # Get the discriminator's prediction on the reals and fakes\n",
    "        disc_fake_pred = disc(fake_image_and_labels.detach())\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "\n",
    "        # Make sure that enough predictions were made\n",
    "        assert len(disc_real_pred) == len(real)\n",
    "        # Make sure that the inputs are different\n",
    "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
    "\n",
    "        # Calculate Discriminator Loss on fakes and reals\n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "\n",
    "        # Get average Discriminator Loss\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step()\n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "        #########################\n",
    "        #  Train Generators\n",
    "        #########################\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        fake_image_and_labels = concat_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if we didn't concatenate wer labels to wer image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "\n",
    "        \"\"\" Now calculate Generator Loss and note that, here, unlike the disc_loss, with\n",
    "        disc_fake_pred, I am passing a vector containing its elements as 1 with torch.ones_like\n",
    "        Because, Generator wants to fool the Discriminator by telling it that all these fake images are actually real, i.e. with value of 1\n",
    "        \"\"\"\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "\n",
    "        ##################################\n",
    "        #  Log Progress and Visualization\n",
    "        #  for each display_step = 50\n",
    "        ##################################\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            # Calculate Generator Mean loss for the latest display_steps (i.e. latest 50 steps)\n",
    "            # list[-x:]   # last x items in the array\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "\n",
    "            # Calculate Discriminator Mean loss for the latest display_steps (i.e. latest 50 steps)\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            print(\n",
    "                f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\"\n",
    "            )\n",
    "\n",
    "            # Plot both the real images and fake generated images\n",
    "            plot_images_from_tensor(fake)\n",
    "            plot_images_from_tensor(real)\n",
    "\n",
    "            step_bins = 20\n",
    "            x_axis = sorted(\n",
    "                [i * step_bins for i in range(len(generator_losses) // step_bins)]\n",
    "                * step_bins\n",
    "            )\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(generator_losses[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Generator Loss\",\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(discriminator_losses[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Discriminator Loss\",\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\"Let Long Training Continue\")\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36686b1-ee93-4701-8c82-dd4cb9a4179c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 11, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_image_and_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fbcdc6e-e387-423a-b262-b185241bea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 11, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_image_and_labels.detach().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b52647-ce4f-4987-ae6d-ed365057b690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc(fake_image_and_labels.detach()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "695cc19e-4ad6-4e4c-85a2-04e72a4cef18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
