{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d921930b-9335-474a-baef-66dd534bc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d29c9b-5b5f-40f0-88fd-d4fc15fcabeb",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46b9c08-f2d9-4f2d-937a-40bf46c67478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data_for_training/train_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    train_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/train_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    train_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/train_images.pkl\", \"rb\") as file:\n",
    "    train_images = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/valid_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    valid_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/valid_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    valid_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/valid_images.pkl\", \"rb\") as file:\n",
    "    valid_images = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/test_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    test_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/test_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    test_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/test_images.pkl\", \"rb\") as file:\n",
    "    test_images = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed2550-450f-434a-8692-7cab21992f0a",
   "metadata": {},
   "source": [
    "## GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4576a-dabb-4f26-9005-b13995c9869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=10, image_channel=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.gen = nn.Sequential(\n",
    "            self._generator_block(input_dim, hidden_dim * 4),\n",
    "            self._generator_block(\n",
    "                hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1\n",
    "            ),\n",
    "            self._generator_block(hidden_dim * 2, hidden_dim),\n",
    "            self._generator_block(\n",
    "                hidden_dim, image_channel, kernel_size=4, final_layer=True\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _generator_block(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        kernel_size=3,\n",
    "        stride=2,\n",
    "        final_layer=False,\n",
    "    ):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    input_channels, output_channels, kernel_size, stride\n",
    "                ),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    input_channels, output_channels, kernel_size, stride\n",
    "                ),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "def create_noise_vector(n_samples, input_dim, device=\"cpu\"):\n",
    "    return torch.randn(n_samples, input_dim, device=device)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_channel=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self._discriminator_block(image_channel, hidden_dim),\n",
    "            self._discriminator_block(hidden_dim, hidden_dim * 2),\n",
    "            self._discriminator_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def _discriminator_block(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        final_layer=False,\n",
    "    ):\n",
    "\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb6d26-e23e-4c4b-ae15-28c1f728b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)  # Set for our testing purposes, please do not change!\n",
    "\n",
    "\n",
    "def plot_images_from_tensor(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    \"\"\"\n",
    "    Plots a grid of images from a given tensor.\n",
    "\n",
    "    The function first scales the image tensor to the range [0, 1]. It then detaches the tensor from the computation\n",
    "    graph and moves it to the CPU if it's not already there. After that, it creates a grid of images and plots the grid.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A 4D tensor containing the images.\n",
    "            The tensor is expected to be in the shape (batch_size, channels, height, width).\n",
    "        num_images (int, optional): The number of images to include in the grid. Default is 25.\n",
    "        size (tuple, optional): The size of a single image in the form of (channels, height, width). Default is (1, 28, 28).\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid. The final grid size is (num_images // nrow, nrow). Default is 5.\n",
    "        show (bool, optional): Determines if the plot should be shown. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        None. The function outputs a plot of a grid of images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the image tensor to [0, 1]\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "\n",
    "    # Detach the tensor from its computation graph and move it to the CPU\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "\n",
    "    # Create a grid of images using the make_grid function from torchvision.utils\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "\n",
    "    # Plot the grid of images\n",
    "    # The permute() function is used to rearrange the dimensions of the grid for plotting\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "\n",
    "    # Show the plot if the 'show' parameter is True\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\"\"\" The reason for doing \"image_grid.permute(1, 2, 0)\"\n",
    "\n",
    "PyTorch modules processing image data expect tensors in the format C × H × W.\n",
    "\n",
    "Whereas PILLow and Matplotlib expect image arrays in the format H × W × C\n",
    "\n",
    "so to use them with matplotlib you need to reshape it\n",
    "to put the channels as the last dimension:\n",
    "\n",
    "I could have used permute() method as well like below\n",
    "\"np.transpose(npimg, (1, 2, 0))\"\n",
    "\n",
    "------------------\n",
    "\n",
    "Tensor.detach() is used to detach a tensor from the current computational graph. It returns a new tensor that doesn't require a gradient.\n",
    "\n",
    "When we don't need a tensor to be traced for the gradient computation, we detach the tensor from the current computational graph.\n",
    "\n",
    "We also need to detach a tensor when we need to move the tensor from GPU to CPU.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize the weights of convolutional and batch normalization layers.\n",
    "\n",
    "    Args:\n",
    "        m (torch.nn.Module): Module instance.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def ohe_vector_from_labels(labels, n_classes):\n",
    "    return F.one_hot(labels, num_classes=n_classes)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.tensor([4, 3, 2, 1, 0])\n",
    "F.one_hot(x, num_classes=6)\n",
    "\n",
    "# Expected result\n",
    "# tensor([[0, 0, 0, 0, 1, 0],\n",
    "#         [0, 0, 0, 1, 0, 0],\n",
    "#         [0, 0, 1, 0, 0, 0],\n",
    "#         [0, 1, 0, 0, 0, 0],\n",
    "#         [1, 0, 0, 0, 0, 0]])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "1. All tensors need to have the same number of dimensions and\n",
    "2. All dimensions except the one that they are concatenated on, need to have the same size. \"\"\"\n",
    "\n",
    "\n",
    "def concat_vectors(x, y):\n",
    "    \"\"\"\n",
    "    Concatenate two tensors along the second dimension.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): First input tensor.\n",
    "        y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Concatenated tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    combined = torch.cat((x.float(), y.float()), 1)\n",
    "    return combined\n",
    "\n",
    "def calculate_input_dim(z_dim, mnist_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate the input dimensions for the generator and discriminator networks.\n",
    "\n",
    "    Args:\n",
    "        z_dim (int): Dimension of the random noise vector (latent space).\n",
    "        mnist_shape (tuple): Shape of the MNIST images, e.g., (1, 28, 28).\n",
    "        n_classes (int): Number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the generator input dimension and discriminator image channel.\n",
    "\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\"\"\"\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "\n",
    "    # mnist_shape[0] is 1 as its grayscale images\n",
    "    discriminator_image_channel = mnist_shape[0] + n_classes\n",
    "\n",
    "    return generator_input_dim, discriminator_image_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d097b-70f2-4b4b-9e9a-063808cc4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from utils import *\n",
    "\n",
    "####################################################\n",
    "def test_weights_init():\n",
    "    # Create a sample model with Conv2d and BatchNorm2d layers\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ConvTranspose2d(16, 3, kernel_size=3),\n",
    "        nn.BatchNorm2d(3)\n",
    "    )\n",
    "\n",
    "    # Initialize the model weights\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # Check the weights of Conv2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "\n",
    "    # Check the weights of BatchNorm2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "            assert torch.allclose(module.bias, torch.tensor(0.0))\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_weights_init()\n",
    "\n",
    "####################################################\n",
    "def test_concat_vectors():\n",
    "    # Create sample input tensors\n",
    "    x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    y = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "    # Perform concatenation\n",
    "    combined = concat_vectors(x, y)\n",
    "\n",
    "    # Check the output type and shape\n",
    "    assert isinstance(combined, torch.Tensor)\n",
    "    assert combined.shape == (2, 6)  # Expected shape after concatenation\n",
    "\n",
    "    # Check the values in the concatenated tensor\n",
    "    expected_combined = torch.tensor([[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]])\n",
    "    assert torch.allclose(combined, expected_combined)\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_concat_vectors()\n",
    "\n",
    "####################################################\n",
    "def test_calculate_input_dim():\n",
    "    # Set up sample inputs\n",
    "    z_dim = 100\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\n",
    "\n",
    "    # Calculate input dimensions\n",
    "    generator_input_dim, discriminator_image_channel = calculate_input_dim(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "    # Check the output types and values\n",
    "    assert isinstance(generator_input_dim, int)\n",
    "    assert generator_input_dim == z_dim + n_classes\n",
    "\n",
    "    assert isinstance(discriminator_image_channel, int)\n",
    "    assert discriminator_image_channel == mnist_shape[0] + n_classes\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_calculate_input_dim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7fc96-cac3-443b-8b2f-6fe78895b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#from conditional_gan import *\n",
    "#from utils import *\n",
    "\n",
    "\n",
    "mnist_shape = (1, 28, 28)\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 64\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = \"cuda\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      unt": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9cdbb2ef-10c0-4368-be7c-218d34c7d372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f2b874c1ee4915bf1a79941c0fa14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/649 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 42, 1, 1]' is invalid for input of size 88608",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m fake_noise \u001b[38;5;241m=\u001b[39m create_noise_vector(cur_batch_size, z_dim, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m noise_and_labels \u001b[38;5;241m=\u001b[39m concat_vectors(fake_noise, one_hot_labels)\n\u001b[0;32m---> 17\u001b[0m fake \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_and_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Make sure that enough images were generated\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#assert len(fake) == len(real)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, noise)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, noise):\n\u001b[0;32m---> 46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnoise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 42, 1, 1]' is invalid for input of size 88608"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for cell, smile, image in tqdm(train_loader):\n",
    "        # Move data to GPU if needed\n",
    "        cell, smile, image = cell.to(device), smile.to(device), image.to(device)\n",
    "\n",
    "        cur_batch_size = len(cell)\n",
    "        z_dim          = cell.shape[1]\n",
    "        \n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        one_hot_labels = smile\n",
    "        \n",
    "        fake_noise = create_noise_vector(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        noise_and_labels = concat_vectors(fake_noise, one_hot_labels)\n",
    "\n",
    "        fake = gen(noise_and_labels)\n",
    "\n",
    "        # Make sure that enough images were generated\n",
    "        #assert len(fake) == len(real)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd5d5609-d788-4ff7-a375-f740ed81020a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2769])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_and_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb3285e4-b1ca-4fa6-bbd5-6b8d2df06ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cur_batch_size = len(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6e17634-63d5-4593-b05a-5db24e6844ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aec99fc6-5f85-4029-a6fc-6241d2d36593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fake_noise.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "442e39a2-4db0-445d-b8df-b9eb85f3bcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c626c651-a232-4705-a3d3-fcaed9df0039",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5b15298c8846fea8ac78bcf55ce04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Dataloader returns the batches and the labels\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcur_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Flatten the batch of real images from the dataset\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torchvision/transforms/functional.py:346\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Normalize a float tensor image with mean and standard deviation.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03mThis transform does not support PIL Image.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m--> 346\u001b[0m     \u001b[43m_log_api_usage_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torchvision/utils.py:643\u001b[0m, in \u001b[0;36m_log_api_usage_once\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, FunctionType):\n\u001b[1;32m    642\u001b[0m     name \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 643\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_api_usage_once\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size\n",
    "        fake_noise = create_noise_vector(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        noise_and_labels = concat_vectors(fake_noise, one_hot_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b0869-eaf9-4dec-b8dd-5c35ed67c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6a727-7707-4b5b-84a3-e44394f5413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data_for_training/train_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    train_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/train_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    train_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/train_images.pkl\", \"rb\") as file:\n",
    "    train_images = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/valid_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    valid_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/valid_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    valid_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/valid_images.pkl\", \"rb\") as file:\n",
    "    valid_images = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/test_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    test_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/test_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    test_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/test_images.pkl\", \"rb\") as file:\n",
    "    test_images = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17efb717-3048-4a5b-8894-a1011ed77eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f201e31-7b55-487d-a485-e0b13a770dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be50f973-b582-4cc9-b59c-7e86efe5c83c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3ea891cac84d308e0cdd33b5320882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n",
      "one_hot_labels  torch.Size([128, 10])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 1, 1])\n",
      "image_one_hot_labels.size  torch.Size([128, 10, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m z_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Dataloader returns the batches and the labels\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcur_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Flatten the batch of real images from the dataset\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torchvision/transforms/functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "z_dim = 64\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        # create one hot encoded vectors from labels and n_classes\n",
    "        one_hot_labels = ohe_vector_from_labels(labels.to(device), n_classes)\n",
    "        print(\"one_hot_labels \", one_hot_labels.size())  # => torch.Size([128, 10])\n",
    "\n",
    "        \"\"\" The above ([128, 10]) need to be converted to ([128, 10, 28, 28])\n",
    "\n",
    "        Because, Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "         1. All tensors need to have the same number of dimensions and\n",
    "         2. All dimensions except the one that they are concatenated on, need to have the same size.\n",
    "\n",
    "        To do that, first I am adding extra dimension with 'None'\n",
    "        the easiest way to add extra dimensions to an array is by using the keyword None,\n",
    "        when indexing at the position to add the extra dimension.\n",
    "        Note, in below with keyword None, I am only adding extra dummy empty dimension\n",
    "\n",
    "        a = torch.rand(1, 2)\n",
    "        ic(a) # => tensor([[0.1749, 0.6387]])\n",
    "        ic(a[None, :]) # => tensor([[[0.1749, 0.6387]]])\n",
    "\n",
    "        a = torch.rand([1,2,3,4])\n",
    "        ic(a.shape) # => torch.Size([1, 2, 3, 4])\n",
    "        ic(a[None, :].shape) # => torch.Size([1, 1, 2, 3, 4])\n",
    "        \"\"\"\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 1, 1])\n",
    "\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "            1, 1, mnist_shape[1], mnist_shape[2]\n",
    "        )\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 28, 28])\n",
    "\n",
    "        #########################\n",
    "        #  Train Discriminator\n",
    "        #########################\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size\n",
    "        fake_noise = create_noise_vector(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "        # Now we can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "\n",
    "        noise_and_labels = concat_vectors(fake_noise, one_hot_labels)\n",
    "        fake = gen(noise_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ada177-7add-49c7-aebd-c74c531442ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        # create one hot encoded vectors from labels and n_classes\n",
    "        one_hot_labels = ohe_vector_from_labels(labels.to(device), n_classes)\n",
    "        print(\"one_hot_labels \", one_hot_labels.size())  # => torch.Size([128, 10])\n",
    "\n",
    "        \"\"\" The above ([128, 10]) need to be converted to ([128, 10, 28, 28])\n",
    "\n",
    "        Because, Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "         1. All tensors need to have the same number of dimensions and\n",
    "         2. All dimensions except the one that they are concatenated on, need to have the same size.\n",
    "\n",
    "        To do that, first I am adding extra dimension with 'None'\n",
    "        the easiest way to add extra dimensions to an array is by using the keyword None,\n",
    "        when indexing at the position to add the extra dimension.\n",
    "        Note, in below with keyword None, I am only adding extra dummy empty dimension\n",
    "\n",
    "        a = torch.rand(1, 2)\n",
    "        ic(a) # => tensor([[0.1749, 0.6387]])\n",
    "        ic(a[None, :]) # => tensor([[[0.1749, 0.6387]]])\n",
    "\n",
    "        a = torch.rand([1,2,3,4])\n",
    "        ic(a.shape) # => torch.Size([1, 2, 3, 4])\n",
    "        ic(a[None, :].shape) # => torch.Size([1, 1, 2, 3, 4])\n",
    "        \"\"\"\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 1, 1])\n",
    "\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "            1, 1, mnist_shape[1], mnist_shape[2]\n",
    "        )\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 28, 28])\n",
    "\n",
    "        #########################\n",
    "        #  Train Discriminator\n",
    "        #########################\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size\n",
    "        fake_noise = create_noise_vector(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "        # Now we can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "\n",
    "        noise_and_labels = concat_vectors(fake_noise, one_hot_labels)\n",
    "        fake = gen(noise_and_labels)\n",
    "\n",
    "        # Make sure that enough images were generated\n",
    "        assert len(fake) == len(real)\n",
    "\n",
    "        # Now we can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels,\n",
    "        #              remember to detach the generator (.detach()) so we do not backpropagate\n",
    "        #              through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "\n",
    "        # Combine the fake images with image_one_hot_labels\n",
    "        fake_image_and_labels = concat_vectors(fake, image_one_hot_labels)\n",
    "\n",
    "        # Combine the real images with image_one_hot_labels\n",
    "        real_image_and_labels = concat_vectors(real, image_one_hot_labels)\n",
    "\n",
    "        # Get the discriminator's prediction on the reals and fakes\n",
    "        disc_fake_pred = disc(fake_image_and_labels.detach())\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "\n",
    "        # Make sure that enough predictions were made\n",
    "        assert len(disc_real_pred) == len(real)\n",
    "        # Make sure that the inputs are different\n",
    "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
    "\n",
    "        # Calculate Discriminator Loss on fakes and reals\n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "\n",
    "        # Get average Discriminator Loss\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step()\n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "        #########################\n",
    "        #  Train Generators\n",
    "        #########################\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        fake_image_and_labels = concat_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if we didn't concatenate wer labels to wer image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "\n",
    "        \"\"\" Now calculate Generator Loss and note that, here, unlike the disc_loss, with\n",
    "        disc_fake_pred, I am passing a vector containing its elements as 1 with torch.ones_like\n",
    "        Because, Generator wants to fool the Discriminator by telling it that all these fake images are actually real, i.e. with value of 1\n",
    "        \"\"\"\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "\n",
    "        ##################################\n",
    "        #  Log Progress and Visualization\n",
    "        #  for each display_step = 50\n",
    "        ##################################\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            # Calculate Generator Mean loss for the latest display_steps (i.e. latest 50 steps)\n",
    "            # list[-x:]   # last x items in the array\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "\n",
    "            # Calculate Discriminator Mean loss for the latest display_steps (i.e. latest 50 steps)\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            print(\n",
    "                f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\"\n",
    "            )\n",
    "\n",
    "            # Plot both the real images and fake generated images\n",
    "            plot_images_from_tensor(fake)\n",
    "            plot_images_from_tensor(real)\n",
    "\n",
    "            step_bins = 20\n",
    "            x_axis = sorted(\n",
    "                [i * step_bins for i in range(len(generator_losses) // step_bins)]\n",
    "                * step_bins\n",
    "            )\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(generator_losses[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Generator Loss\",\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(discriminator_losses[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Discriminator Loss\",\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\"Let Long Training Continue\")\n",
    "        cur_step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
