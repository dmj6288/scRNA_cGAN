{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d921930b-9335-474a-baef-66dd534bc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38dc4893-dde4-43f9-9e4c-43003965a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cell_embeddings = pd.read_csv(\"../ZafrensData/latent_embeddings_zel024_1K.csv\")\n",
    "cell_embeddings = np.array(cell_embeddings)\n",
    "\n",
    "with open(\"../data_for_training/matched_metadata_125_zel024.pkl\", \"rb\") as file:\n",
    "    matched_metadata = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/smiles_embedding_matrix.pkl\", \"rb\") as file:\n",
    "    smiles_embeddings = pkl.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4f752a-339a-48e1-bb13-92f3fd405129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../ZafrensData/final_dataset/images_zel024_125_20K.pkl\", \"rb\") as file:\n",
    "    images_34K = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ba0d0c-9e91-4f8e-a4a8-8e5b360af638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 20756\n",
      "Test set: 4448\n",
      "Validation set: 4448\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_cleaned = matched_metadata.dropna()\n",
    "\n",
    "# Assume 'df' is your DataFrame\n",
    "# First split: 70% train, 30% temporary (test + validation)\n",
    "train, temp = train_test_split(df_cleaned, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: split the temporary set equally into test and validation (15% each)\n",
    "test, validation = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes:\n",
    "print(\"Train set:\", len(train))\n",
    "print(\"Test set:\", len(test))\n",
    "print(\"Validation set:\", len(validation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91eb81f-cd8d-45b3-a8a8-d4541340d9a4",
   "metadata": {},
   "source": [
    "#### Training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2873c87d-9f36-4b11-a725-9b84dbd430ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4630     15873\n",
       "25931     2571\n",
       "1654      1640\n",
       "34607     4455\n",
       "19421     7866\n",
       "         ...  \n",
       "25373    11182\n",
       "6267     14673\n",
       "860       2947\n",
       "18675     7706\n",
       "27810    18720\n",
       "Name: image_index, Length: 20756, dtype: Int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['image_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd927cda-f3c1-4a79-af6f-2545f5fb16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_cell_embeddings = cell_embeddings[list(train['cell_index'].astype('Int64')), :]\n",
    "train_smiles_embeddings = smiles_embeddings[list(train['smiles_index'].astype('Int64')), :]\n",
    "train_images = images_34K[list(train['image_index'].astype('Int64')), :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e34cd8-4503-4677-8234-850308ed1213",
   "metadata": {},
   "source": [
    "#### Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c541c4-a462-4835-b8db-c004f2668b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_cell_embeddings = cell_embeddings[list(validation['cell_index'].astype('Int64')), :]\n",
    "valid_smiles_embeddings = smiles_embeddings[list(validation['smiles_index'].astype('Int64')), :]\n",
    "valid_images = images_34K[list(validation['image_index'].astype('Int64')), :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a6447-7e3d-40c4-8afe-1c6b6d810eda",
   "metadata": {},
   "source": [
    "\n",
    "#### Test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f329763-1c21-4034-aace-2332fe791b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_cell_embeddings = cell_embeddings[list(test['cell_index'].astype('Int64')), :]\n",
    "test_smiles_embeddings = smiles_embeddings[list(test['smiles_index'].astype('Int64')), :]\n",
    "test_images = images_34K[list(test['image_index'].astype('Int64')), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46b9c08-f2d9-4f2d-937a-40bf46c67478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed_ImgSize125/train_cell_embeddings.pkl\", \"wb\") as file:\n",
    "    pkl.dump(train_cell_embeddings, file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed_ImgSize125/train_smiles_embeddings.pkl\", \"wb\") as file:\n",
    "    pkl.dump(train_smiles_embeddings, file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed_ImgSize125/train_images.pkl\", \"wb\") as file:\n",
    "    pkl.dump(train_images, file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed_ImgSize125/valid_cell_embeddings.pkl\", \"wb\") as file:\n",
    "    pkl.dump(valid_cell_embeddings, file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed_ImgSize125/valid_smiles_embeddings.pkl\", \"wb\") as file:\n",
    "    pkl.dump(valid_smiles_embeddings, file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed_ImgSize125/valid_images.pkl\", \"wb\") as file:\n",
    "    pkl.dump(valid_images, file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed_ImgSize125/test_cell_embeddings.pkl\", \"wb\") as file:\n",
    "    pkl.dump(test_cell_embeddings, file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed_ImgSize125/test_smiles_embeddings.pkl\", \"wb\") as file:\n",
    "    pkl.dump(test_smiles_embeddings, file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed_ImgSize125/test_images.pkl\", \"wb\") as file:\n",
    "    pkl.dump(test_images, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed2550-450f-434a-8692-7cab21992f0a",
   "metadata": {},
   "source": [
    "## GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4576a-dabb-4f26-9005-b13995c9869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=10, image_channel=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.gen = nn.Sequential(\n",
    "            self._generator_block(input_dim, hidden_dim * 4),\n",
    "            self._generator_block(\n",
    "                hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1\n",
    "            ),\n",
    "            self._generator_block(hidden_dim * 2, hidden_dim),\n",
    "            self._generator_block(\n",
    "                hidden_dim, image_channel, kernel_size=4, final_layer=True\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _generator_block(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        kernel_size=3,\n",
    "        stride=2,\n",
    "        final_layer=False,\n",
    "    ):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    input_channels, output_channels, kernel_size, stride\n",
    "                ),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    input_channels, output_channels, kernel_size, stride\n",
    "                ),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "def create_noise_vector(n_samples, input_dim, device=\"cpu\"):\n",
    "    return torch.randn(n_samples, input_dim, device=device)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_channel=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self._discriminator_block(image_channel, hidden_dim),\n",
    "            self._discriminator_block(hidden_dim, hidden_dim * 2),\n",
    "            self._discriminator_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def _discriminator_block(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        final_layer=False,\n",
    "    ):\n",
    "\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb6d26-e23e-4c4b-ae15-28c1f728b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)  # Set for our testing purposes, please do not change!\n",
    "\n",
    "\n",
    "def plot_images_from_tensor(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    \"\"\"\n",
    "    Plots a grid of images from a given tensor.\n",
    "\n",
    "    The function first scales the image tensor to the range [0, 1]. It then detaches the tensor from the computation\n",
    "    graph and moves it to the CPU if it's not already there. After that, it creates a grid of images and plots the grid.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A 4D tensor containing the images.\n",
    "            The tensor is expected to be in the shape (batch_size, channels, height, width).\n",
    "        num_images (int, optional): The number of images to include in the grid. Default is 25.\n",
    "        size (tuple, optional): The size of a single image in the form of (channels, height, width). Default is (1, 28, 28).\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid. The final grid size is (num_images // nrow, nrow). Default is 5.\n",
    "        show (bool, optional): Determines if the plot should be shown. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        None. The function outputs a plot of a grid of images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the image tensor to [0, 1]\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "\n",
    "    # Detach the tensor from its computation graph and move it to the CPU\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "\n",
    "    # Create a grid of images using the make_grid function from torchvision.utils\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "\n",
    "    # Plot the grid of images\n",
    "    # The permute() function is used to rearrange the dimensions of the grid for plotting\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "\n",
    "    # Show the plot if the 'show' parameter is True\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\"\"\" The reason for doing \"image_grid.permute(1, 2, 0)\"\n",
    "\n",
    "PyTorch modules processing image data expect tensors in the format C × H × W.\n",
    "\n",
    "Whereas PILLow and Matplotlib expect image arrays in the format H × W × C\n",
    "\n",
    "so to use them with matplotlib you need to reshape it\n",
    "to put the channels as the last dimension:\n",
    "\n",
    "I could have used permute() method as well like below\n",
    "\"np.transpose(npimg, (1, 2, 0))\"\n",
    "\n",
    "------------------\n",
    "\n",
    "Tensor.detach() is used to detach a tensor from the current computational graph. It returns a new tensor that doesn't require a gradient.\n",
    "\n",
    "When we don't need a tensor to be traced for the gradient computation, we detach the tensor from the current computational graph.\n",
    "\n",
    "We also need to detach a tensor when we need to move the tensor from GPU to CPU.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize the weights of convolutional and batch normalization layers.\n",
    "\n",
    "    Args:\n",
    "        m (torch.nn.Module): Module instance.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def ohe_vector_from_labels(labels, n_classes):\n",
    "    return F.one_hot(labels, num_classes=n_classes)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.tensor([4, 3, 2, 1, 0])\n",
    "F.one_hot(x, num_classes=6)\n",
    "\n",
    "# Expected result\n",
    "# tensor([[0, 0, 0, 0, 1, 0],\n",
    "#         [0, 0, 0, 1, 0, 0],\n",
    "#         [0, 0, 1, 0, 0, 0],\n",
    "#         [0, 1, 0, 0, 0, 0],\n",
    "#         [1, 0, 0, 0, 0, 0]])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "1. All tensors need to have the same number of dimensions and\n",
    "2. All dimensions except the one that they are concatenated on, need to have the same size. \"\"\"\n",
    "\n",
    "\n",
    "def concat_vectors(x, y):\n",
    "    \"\"\"\n",
    "    Concatenate two tensors along the second dimension.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): First input tensor.\n",
    "        y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Concatenated tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    combined = torch.cat((x.float(), y.float()), 1)\n",
    "    return combined\n",
    "\n",
    "def calculate_input_dim(z_dim, mnist_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate the input dimensions for the generator and discriminator networks.\n",
    "\n",
    "    Args:\n",
    "        z_dim (int): Dimension of the random noise vector (latent space).\n",
    "        mnist_shape (tuple): Shape of the MNIST images, e.g., (1, 28, 28).\n",
    "        n_classes (int): Number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the generator input dimension and discriminator image channel.\n",
    "\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\"\"\"\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "\n",
    "    # mnist_shape[0] is 1 as its grayscale images\n",
    "    discriminator_image_channel = mnist_shape[0] + n_classes\n",
    "\n",
    "    return generator_input_dim, discriminator_image_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d097b-70f2-4b4b-9e9a-063808cc4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from utils import *\n",
    "\n",
    "####################################################\n",
    "def test_weights_init():\n",
    "    # Create a sample model with Conv2d and BatchNorm2d layers\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ConvTranspose2d(16, 3, kernel_size=3),\n",
    "        nn.BatchNorm2d(3)\n",
    "    )\n",
    "\n",
    "    # Initialize the model weights\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # Check the weights of Conv2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "\n",
    "    # Check the weights of BatchNorm2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "            assert torch.allclose(module.bias, torch.tensor(0.0))\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_weights_init()\n",
    "\n",
    "####################################################\n",
    "def test_concat_vectors():\n",
    "    # Create sample input tensors\n",
    "    x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    y = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "    # Perform concatenation\n",
    "    combined = concat_vectors(x, y)\n",
    "\n",
    "    # Check the output type and shape\n",
    "    assert isinstance(combined, torch.Tensor)\n",
    "    assert combined.shape == (2, 6)  # Expected shape after concatenation\n",
    "\n",
    "    # Check the values in the concatenated tensor\n",
    "    expected_combined = torch.tensor([[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]])\n",
    "    assert torch.allclose(combined, expected_combined)\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_concat_vectors()\n",
    "\n",
    "####################################################\n",
    "def test_calculate_input_dim():\n",
    "    # Set up sample inputs\n",
    "    z_dim = 100\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\n",
    "\n",
    "    # Calculate input dimensions\n",
    "    generator_input_dim, discriminator_image_channel = calculate_input_dim(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "    # Check the output types and values\n",
    "    assert isinstance(generator_input_dim, int)\n",
    "    assert generator_input_dim == z_dim + n_classes\n",
    "\n",
    "    assert isinstance(discriminator_image_channel, int)\n",
    "    assert discriminator_image_channel == mnist_shape[0] + n_classes\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_calculate_input_dim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7fc96-cac3-443b-8b2f-6fe78895b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#from conditional_gan import *\n",
    "#from utils import *\n",
    "\n",
    "\n",
    "mnist_shape = (1, 28, 28)\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 64\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = \"cuda\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
