{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d921930b-9335-474a-baef-66dd534bc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d29c9b-5b5f-40f0-88fd-d4fc15fcabeb",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46b9c08-f2d9-4f2d-937a-40bf46c67478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed/train_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    train_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed/train_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    train_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed/train_images.pkl\", \"rb\") as file:\n",
    "    train_images = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed/valid_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    valid_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed/valid_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    valid_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed/valid_images.pkl\", \"rb\") as file:\n",
    "    valid_images = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed/test_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    test_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed/test_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    test_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/OneK_cellEmbed/test_images.pkl\", \"rb\") as file:\n",
    "    test_images = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed2550-450f-434a-8692-7cab21992f0a",
   "metadata": {},
   "source": [
    "## GAN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31f4bd-0089-4ab7-9664-3bc460823a2a",
   "metadata": {},
   "source": [
    "        self.gen = nn.Sequential(\n",
    "            self._generator_block(input_dim, hidden_dim * 8, kernel_size=5, stride=5),   # (1x1) → (5x5)\n",
    "            self._generator_block(hidden_dim * 8, hidden_dim * 4, kernel_size=4, stride=2, padding=1),   # (5x5) → (10x10)\n",
    "            self._generator_block(hidden_dim * 4, hidden_dim * 2, kernel_size=3, stride=2, padding=0),   # (10x10) → (21x21)\n",
    "            self._generator_block(hidden_dim * 2, hidden_dim, kernel_size=4, stride=2, padding=1),   # (21x21) → (42x42)\n",
    "            self._generator_block(hidden_dim, hidden_dim // 2, kernel_size=3, stride=2, padding=1),   # (42x42) → (85x85)\n",
    "            self._generator_block(hidden_dim // 2, hidden_dim // 4, kernel_size=4, stride=2, padding=1),   # (85x85) → (170x170)\n",
    "            self._generator_block(hidden_dim // 4, hidden_dim // 8, kernel_size=3, stride=2, padding=1),   # (170x170) → (255x255)\n",
    "            self._generator_block(hidden_dim // 8, hidden_dim // 16, kernel_size=4, stride=1, padding=0),  # (255x255) → (272x272)\n",
    "            self._generator_block(hidden_dim // 16, hidden_dim // 32, kernel_size=4, stride=1, padding=0),  # (272x272) → (300x300)\n",
    "            self._generator_block(hidden_dim // 32, image_channel, kernel_size=4, stride=1, padding=0, final_layer=True),  # (300x300) → (340x340)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd4576a-dabb-4f26-9005-b13995c9869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=2769, image_channel=1, hidden_dim=256):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.gen = nn.Sequential(\n",
    "            self._generator_block(input_dim, hidden_dim * 8, kernel_size=5, stride=5),   # (1x1) → (5x5)\n",
    "            self._generator_block(hidden_dim * 8, hidden_dim * 4, kernel_size=4, stride=2, padding=1),   # (5x5) → (10x10)\n",
    "            self._generator_block(hidden_dim * 4, hidden_dim * 2, kernel_size=3, stride=2, padding=0),   # (10x10) → (21x21)\n",
    "            self._generator_block(hidden_dim * 2, hidden_dim, kernel_size=4, stride=2, padding=1),   # (21x21) → (42x42)\n",
    "            self._generator_block(hidden_dim, hidden_dim // 2, kernel_size=3, stride=2, padding=1),   # (42x42) → (85x85)\n",
    "            self._generator_block(hidden_dim // 2, hidden_dim // 4, kernel_size=4, stride=2, padding=1),   # (85x85) → (170x170)\n",
    "            self._generator_block(hidden_dim // 4, hidden_dim // 8, kernel_size=3, stride=2, padding=1),   # (170x170) → (255x255)\n",
    "            self._generator_block(hidden_dim // 8, hidden_dim // 16, kernel_size=4, stride=1, padding=0),  # (255x255) → (272x272)\n",
    "            self._generator_block(hidden_dim // 16, hidden_dim // 32, kernel_size=4, stride=1, padding=0),  # (272x272) → (300x300)\n",
    "            self._generator_block(hidden_dim // 32, image_channel, kernel_size=4, stride=1, padding=0, final_layer=True),  # (300x300) → (340x340)\n",
    "        )\n",
    "\n",
    "    def _generator_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=0, final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "def create_noise_vector(n_samples, input_dim, device=\"cpu\"):\n",
    "    return torch.randn(n_samples, input_dim, device=device)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_channel=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self._discriminator_block(image_channel, hidden_dim),\n",
    "            self._discriminator_block(hidden_dim, hidden_dim * 2),\n",
    "            self._discriminator_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def _discriminator_block(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        final_layer=False,\n",
    "    ):\n",
    "\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aadb6d26-e23e-4c4b-ae15-28c1f728b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)  # Set for our testing purposes, please do not change!\n",
    "\n",
    "\n",
    "def plot_images_from_tensor(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    \"\"\"\n",
    "    Plots a grid of images from a given tensor.\n",
    "\n",
    "    The function first scales the image tensor to the range [0, 1]. It then detaches the tensor from the computation\n",
    "    graph and moves it to the CPU if it's not already there. After that, it creates a grid of images and plots the grid.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A 4D tensor containing the images.\n",
    "            The tensor is expected to be in the shape (batch_size, channels, height, width).\n",
    "        num_images (int, optional): The number of images to include in the grid. Default is 25.\n",
    "        size (tuple, optional): The size of a single image in the form of (channels, height, width). Default is (1, 28, 28).\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid. The final grid size is (num_images // nrow, nrow). Default is 5.\n",
    "        show (bool, optional): Determines if the plot should be shown. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        None. The function outputs a plot of a grid of images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the image tensor to [0, 1]\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "\n",
    "    # Detach the tensor from its computation graph and move it to the CPU\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "\n",
    "    # Create a grid of images using the make_grid function from torchvision.utils\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "\n",
    "    # Plot the grid of images\n",
    "    # The permute() function is used to rearrange the dimensions of the grid for plotting\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "\n",
    "    # Show the plot if the 'show' parameter is True\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\"\"\" The reason for doing \"image_grid.permute(1, 2, 0)\"\n",
    "\n",
    "PyTorch modules processing image data expect tensors in the format C × H × W.\n",
    "\n",
    "Whereas PILLow and Matplotlib expect image arrays in the format H × W × C\n",
    "\n",
    "so to use them with matplotlib you need to reshape it\n",
    "to put the channels as the last dimension:\n",
    "\n",
    "I could have used permute() method as well like below\n",
    "\"np.transpose(npimg, (1, 2, 0))\"\n",
    "\n",
    "------------------\n",
    "\n",
    "Tensor.detach() is used to detach a tensor from the current computational graph. It returns a new tensor that doesn't require a gradient.\n",
    "\n",
    "When we don't need a tensor to be traced for the gradient computation, we detach the tensor from the current computational graph.\n",
    "\n",
    "We also need to detach a tensor when we need to move the tensor from GPU to CPU.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize the weights of convolutional and batch normalization layers.\n",
    "\n",
    "    Args:\n",
    "        m (torch.nn.Module): Module instance.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def ohe_vector_from_labels(labels, n_classes):\n",
    "    return F.one_hot(labels, num_classes=n_classes)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.tensor([4, 3, 2, 1, 0])\n",
    "F.one_hot(x, num_classes=6)\n",
    "\n",
    "# Expected result\n",
    "# tensor([[0, 0, 0, 0, 1, 0],\n",
    "#         [0, 0, 0, 1, 0, 0],\n",
    "#         [0, 0, 1, 0, 0, 0],\n",
    "#         [0, 1, 0, 0, 0, 0],\n",
    "#         [1, 0, 0, 0, 0, 0]])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "1. All tensors need to have the same number of dimensions and\n",
    "2. All dimensions except the one that they are concatenated on, need to have the same size. \"\"\"\n",
    "\n",
    "\n",
    "def concat_vectors(x, y):\n",
    "    \"\"\"\n",
    "    Concatenate two tensors along the second dimension.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): First input tensor.\n",
    "        y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Concatenated tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    combined = torch.cat((x.float(), y.float()), 1)\n",
    "    return combined\n",
    "\n",
    "def calculate_input_dim(z_dim, mnist_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate the input dimensions for the generator and discriminator networks.\n",
    "\n",
    "    Args:\n",
    "        z_dim (int): Dimension of the random noise vector (latent space).\n",
    "        mnist_shape (tuple): Shape of the MNIST images, e.g., (1, 28, 28).\n",
    "        n_classes (int): Number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the generator input dimension and discriminator image channel.\n",
    "\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\"\"\"\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "\n",
    "    # mnist_shape[0] is 1 as its grayscale images\n",
    "    discriminator_image_channel = mnist_shape[0] + n_classes\n",
    "\n",
    "    return generator_input_dim, discriminator_image_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25d097b-70f2-4b4b-9e9a-063808cc4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from utils import *\n",
    "\n",
    "####################################################\n",
    "def test_weights_init():\n",
    "    # Create a sample model with Conv2d and BatchNorm2d layers\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ConvTranspose2d(16, 3, kernel_size=3),\n",
    "        nn.BatchNorm2d(3)\n",
    "    )\n",
    "\n",
    "    # Initialize the model weights\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # Check the weights of Conv2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "\n",
    "    # Check the weights of BatchNorm2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "            assert torch.allclose(module.bias, torch.tensor(0.0))\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_weights_init()\n",
    "\n",
    "####################################################\n",
    "def test_concat_vectors():\n",
    "    # Create sample input tensors\n",
    "    x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    y = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "    # Perform concatenation\n",
    "    combined = concat_vectors(x, y)\n",
    "\n",
    "    # Check the output type and shape\n",
    "    assert isinstance(combined, torch.Tensor)\n",
    "    assert combined.shape == (2, 6)  # Expected shape after concatenation\n",
    "\n",
    "    # Check the values in the concatenated tensor\n",
    "    expected_combined = torch.tensor([[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]])\n",
    "    assert torch.allclose(combined, expected_combined)\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_concat_vectors()\n",
    "\n",
    "####################################################\n",
    "def test_calculate_input_dim():\n",
    "    # Set up sample inputs\n",
    "    z_dim = 100\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\n",
    "\n",
    "    # Calculate input dimensions\n",
    "    generator_input_dim, discriminator_image_channel = calculate_input_dim(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "    # Check the output types and values\n",
    "    assert isinstance(generator_input_dim, int)\n",
    "    assert generator_input_dim == z_dim + n_classes\n",
    "\n",
    "    assert isinstance(discriminator_image_channel, int)\n",
    "    assert discriminator_image_channel == mnist_shape[0] + n_classes\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_calculate_input_dim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c7fc96-cac3-443b-8b2f-6fe78895b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#from conditional_gan import *\n",
    "#from utils import *\n",
    "\n",
    "mnist_shape = (1, 340, 340)\n",
    "n_classes = 768\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 1\n",
    "z_dim = 2001\n",
    "display_step = 500\n",
    "batch_size = 32\n",
    "lr = 0.0002\n",
    "device = \"cuda\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a08800b-adcd-486f-b7e6-31b3c4361bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataloader = DataLoader(\n",
    "#    MNIST(\n",
    "#        \"/home/dennis00/scRNA_GAN/MNIST\", download=True, transform=transform\n",
    "#    ),\n",
    "#    batch_size=batch_size,\n",
    "#    shuffle=True,\n",
    "#)\n",
    "\n",
    "generator_input_dim, discriminator_image_channel = calculate_input_dim(\n",
    "    z_dim, mnist_shape, n_classes\n",
    ")\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "disc = Discriminator(image_channel=discriminator_image_channel).to(device)\n",
    "\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "gen  = gen.apply(weights_init)\n",
    "\n",
    "disc = disc.apply(weights_init)\n",
    "\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69795d4-58ef-440e-847b-53b8071f5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, cell_embeddings, smiles_embeddings, images):\n",
    "        self.cell_embeddings = torch.tensor(cell_embeddings, dtype=torch.float32)\n",
    "        self.smiles_embeddings = torch.tensor(smiles_embeddings, dtype=torch.float32)\n",
    "        self.images = torch.tensor(images, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cell_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.cell_embeddings[idx], \n",
    "            self.smiles_embeddings[idx], \n",
    "            self.images[idx]\n",
    "        )\n",
    "\n",
    "# Example usage for train set\n",
    "train_dataset = MultiModalDataset(train_cell_embeddings, train_smiles_embeddings, train_images)\n",
    "\n",
    "# Same for validation and test sets\n",
    "val_dataset = MultiModalDataset(valid_cell_embeddings, valid_smiles_embeddings, valid_images)\n",
    "test_dataset = MultiModalDataset(test_cell_embeddings, test_smiles_embeddings, test_images)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06490a67-0bfd-447b-b2e4-cc5aa8bb16d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4950aed-c7d7-4a55-9682-938dae93e02c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreal\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'real' is not defined"
     ]
    }
   ],
   "source": [
    "real.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e27a0-2589-4e20-bc73-968b5b26e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2a38a-d431-4fea-aa8d-15484e942514",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_and_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e27e4-c86a-4dfa-8d32-27a8ae6dda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5138b-1ff6-4d2b-9373-d3f11d32effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5233f-ce7d-4c62-add7-e93dd9ca8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2ed5e-3651-4a5d-86e6-89f77389456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cdbb2ef-10c0-4368-be7c-218d34c7d372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042be13f33c64fd39143021b18ef28c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/649 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_labels  torch.Size([32, 768])\n",
      "image_one_hot_labels.size  torch.Size([32, 768, 1, 1])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 10.60 GiB. GPU 0 has a total capacity of 31.74 GiB of which 7.14 GiB is free. Including non-PyTorch memory, this process has 24.60 GiB memory in use. Of the allocated memory 24.20 GiB is allocated by PyTorch, and 32.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m fake_image_and_labels \u001b[38;5;241m=\u001b[39m concat_vectors(fake, image_one_hot_labels)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Combine the real images with image_one_hot_labels\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m real_image_and_labels \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_one_hot_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Get the discriminator's prediction on the reals and fakes\u001b[39;00m\n\u001b[1;32m     38\u001b[0m disc_fake_pred \u001b[38;5;241m=\u001b[39m disc(fake_image_and_labels\u001b[38;5;241m.\u001b[39mdetach())\n",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m, in \u001b[0;36mconcat_vectors\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconcat_vectors\u001b[39m(x, y):\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Concatenate two tensors along the second dimension.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 10.60 GiB. GPU 0 has a total capacity of 31.74 GiB of which 7.14 GiB is free. Including non-PyTorch memory, this process has 24.60 GiB memory in use. Of the allocated memory 24.20 GiB is allocated by PyTorch, and 32.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for cell, smile, real in tqdm(train_loader):\n",
    "        # Move data to GPU if needed\n",
    "        cell, smile, real = cell.to(device), smile.to(device), real.to(device)\n",
    "\n",
    "        real = real.unsqueeze(1)  # Adds a channel dimension at index 1\n",
    "\n",
    "        cur_batch_size = len(cell)\n",
    "        \n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        one_hot_labels = smile\n",
    "        print(\"one_hot_labels \", one_hot_labels.size())\n",
    "        fake_noise = create_noise_vector(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        noise_and_labels = concat_vectors(fake_noise, one_hot_labels)\n",
    "\n",
    "        fake = gen(noise_and_labels)\n",
    "\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 1, 1])\n",
    "\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "            1, 1, real.shape[2], real.shape[2]\n",
    "        )\n",
    "\n",
    "        # Make sure that enough images were generated\n",
    "        assert len(fake) == len(real)\n",
    "        \n",
    "        fake_image_and_labels = concat_vectors(fake, image_one_hot_labels)\n",
    "\n",
    "        # Combine the real images with image_one_hot_labels\n",
    "        real_image_and_labels = concat_vectors(real, image_one_hot_labels)\n",
    "\n",
    "        # Get the discriminator's prediction on the reals and fakes\n",
    "        disc_fake_pred = disc(fake_image_and_labels.detach())\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "\n",
    "        # Make sure that enough predictions were made\n",
    "        assert len(disc_real_pred) == len(real)\n",
    "        # Make sure that the inputs are different\n",
    "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "494c2393-a5e4-4e80-a397-1f1840f23560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 340, 340])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b11aa95-8c27-4ade-a5dc-7b9d25e36554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 340, 340])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b185855-928d-4405-8740-f9c801e0798a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768, 340, 340])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_one_hot_labels.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
