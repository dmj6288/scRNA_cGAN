{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d921930b-9335-474a-baef-66dd534bc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d29c9b-5b5f-40f0-88fd-d4fc15fcabeb",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46b9c08-f2d9-4f2d-937a-40bf46c67478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data_for_training/train_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    train_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/train_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    train_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/train_images.pkl\", \"rb\") as file:\n",
    "    train_images = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/valid_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    valid_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/valid_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    valid_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/valid_images.pkl\", \"rb\") as file:\n",
    "    valid_images = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/test_cell_embeddings.pkl\", \"rb\") as file:\n",
    "    test_cell_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/test_smiles_embeddings.pkl\", \"rb\") as file:\n",
    "    test_smiles_embeddings = pkl.load(file)\n",
    "\n",
    "with open(\"../data_for_training/test_images.pkl\", \"rb\") as file:\n",
    "    test_images = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed2550-450f-434a-8692-7cab21992f0a",
   "metadata": {},
   "source": [
    "## GAN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31f4bd-0089-4ab7-9664-3bc460823a2a",
   "metadata": {},
   "source": [
    "        self.gen = nn.Sequential(\n",
    "            self._generator_block(input_dim, hidden_dim * 8, kernel_size=5, stride=5),   # (1x1) → (5x5)\n",
    "            self._generator_block(hidden_dim * 8, hidden_dim * 4, kernel_size=4, stride=2, padding=1),   # (5x5) → (10x10)\n",
    "            self._generator_block(hidden_dim * 4, hidden_dim * 2, kernel_size=3, stride=2, padding=0),   # (10x10) → (21x21)\n",
    "            self._generator_block(hidden_dim * 2, hidden_dim, kernel_size=4, stride=2, padding=1),   # (21x21) → (42x42)\n",
    "            self._generator_block(hidden_dim, hidden_dim // 2, kernel_size=3, stride=2, padding=1),   # (42x42) → (85x85)\n",
    "            self._generator_block(hidden_dim // 2, hidden_dim // 4, kernel_size=4, stride=2, padding=1),   # (85x85) → (170x170)\n",
    "            self._generator_block(hidden_dim // 4, hidden_dim // 8, kernel_size=3, stride=2, padding=1),   # (170x170) → (255x255)\n",
    "            self._generator_block(hidden_dim // 8, hidden_dim // 16, kernel_size=4, stride=1, padding=0),  # (255x255) → (272x272)\n",
    "            self._generator_block(hidden_dim // 16, hidden_dim // 32, kernel_size=4, stride=1, padding=0),  # (272x272) → (300x300)\n",
    "            self._generator_block(hidden_dim // 32, image_channel, kernel_size=4, stride=1, padding=0, final_layer=True),  # (300x300) → (340x340)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7bd4576a-dabb-4f26-9005-b13995c9869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=2769, image_channel=1, hidden_dim=256):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.gen = nn.Sequential(\n",
    "            self._generator_block(input_dim, hidden_dim * 8, kernel_size=5, stride=5),   # (1x1) → (5x5)\n",
    "            self._generator_block(hidden_dim * 8, hidden_dim * 4, kernel_size=4, stride=2, padding=1),   # (5x5) → (10x10)\n",
    "            self._generator_block(hidden_dim * 4, hidden_dim * 2, kernel_size=3, stride=2, padding=0),   # (10x10) → (21x21)\n",
    "            self._generator_block(hidden_dim * 2, hidden_dim, kernel_size=4, stride=2, padding=1),   # (21x21) → (42x42)\n",
    "            self._generator_block(hidden_dim, hidden_dim // 2, kernel_size=3, stride=2, padding=1),   # (42x42) → (85x85)\n",
    "            self._generator_block(hidden_dim // 2, hidden_dim // 4, kernel_size=4, stride=2, padding=1),   # (85x85) → (170x170)\n",
    "            self._generator_block(hidden_dim // 4, hidden_dim // 8, kernel_size=3, stride=2, padding=1),   # (170x170) → (255x255)\n",
    "            self._generator_block(hidden_dim // 8, hidden_dim // 16, kernel_size=4, stride=1, padding=0),  # (255x255) → (272x272)\n",
    "            self._generator_block(hidden_dim // 16, hidden_dim // 32, kernel_size=4, stride=1, padding=0),  # (272x272) → (300x300)\n",
    "            self._generator_block(hidden_dim // 32, image_channel, kernel_size=4, stride=1, padding=0, final_layer=True),  # (300x300) → (340x340)\n",
    "        )\n",
    "\n",
    "    def _generator_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=0, final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "def create_noise_vector(n_samples, input_dim, device=\"cpu\"):\n",
    "    return torch.randn(n_samples, input_dim, device=device)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_channel=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self._discriminator_block(image_channel, hidden_dim),\n",
    "            self._discriminator_block(hidden_dim, hidden_dim * 2),\n",
    "            self._discriminator_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def _discriminator_block(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        final_layer=False,\n",
    "    ):\n",
    "\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aadb6d26-e23e-4c4b-ae15-28c1f728b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)  # Set for our testing purposes, please do not change!\n",
    "\n",
    "\n",
    "def plot_images_from_tensor(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    \"\"\"\n",
    "    Plots a grid of images from a given tensor.\n",
    "\n",
    "    The function first scales the image tensor to the range [0, 1]. It then detaches the tensor from the computation\n",
    "    graph and moves it to the CPU if it's not already there. After that, it creates a grid of images and plots the grid.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A 4D tensor containing the images.\n",
    "            The tensor is expected to be in the shape (batch_size, channels, height, width).\n",
    "        num_images (int, optional): The number of images to include in the grid. Default is 25.\n",
    "        size (tuple, optional): The size of a single image in the form of (channels, height, width). Default is (1, 28, 28).\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid. The final grid size is (num_images // nrow, nrow). Default is 5.\n",
    "        show (bool, optional): Determines if the plot should be shown. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        None. The function outputs a plot of a grid of images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the image tensor to [0, 1]\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "\n",
    "    # Detach the tensor from its computation graph and move it to the CPU\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "\n",
    "    # Create a grid of images using the make_grid function from torchvision.utils\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "\n",
    "    # Plot the grid of images\n",
    "    # The permute() function is used to rearrange the dimensions of the grid for plotting\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "\n",
    "    # Show the plot if the 'show' parameter is True\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\"\"\" The reason for doing \"image_grid.permute(1, 2, 0)\"\n",
    "\n",
    "PyTorch modules processing image data expect tensors in the format C × H × W.\n",
    "\n",
    "Whereas PILLow and Matplotlib expect image arrays in the format H × W × C\n",
    "\n",
    "so to use them with matplotlib you need to reshape it\n",
    "to put the channels as the last dimension:\n",
    "\n",
    "I could have used permute() method as well like below\n",
    "\"np.transpose(npimg, (1, 2, 0))\"\n",
    "\n",
    "------------------\n",
    "\n",
    "Tensor.detach() is used to detach a tensor from the current computational graph. It returns a new tensor that doesn't require a gradient.\n",
    "\n",
    "When we don't need a tensor to be traced for the gradient computation, we detach the tensor from the current computational graph.\n",
    "\n",
    "We also need to detach a tensor when we need to move the tensor from GPU to CPU.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize the weights of convolutional and batch normalization layers.\n",
    "\n",
    "    Args:\n",
    "        m (torch.nn.Module): Module instance.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def ohe_vector_from_labels(labels, n_classes):\n",
    "    return F.one_hot(labels, num_classes=n_classes)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.tensor([4, 3, 2, 1, 0])\n",
    "F.one_hot(x, num_classes=6)\n",
    "\n",
    "# Expected result\n",
    "# tensor([[0, 0, 0, 0, 1, 0],\n",
    "#         [0, 0, 0, 1, 0, 0],\n",
    "#         [0, 0, 1, 0, 0, 0],\n",
    "#         [0, 1, 0, 0, 0, 0],\n",
    "#         [1, 0, 0, 0, 0, 0]])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "1. All tensors need to have the same number of dimensions and\n",
    "2. All dimensions except the one that they are concatenated on, need to have the same size. \"\"\"\n",
    "\n",
    "\n",
    "def concat_vectors(x, y):\n",
    "    \"\"\"\n",
    "    Concatenate two tensors along the second dimension.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): First input tensor.\n",
    "        y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Concatenated tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    combined = torch.cat((x.float(), y.float()), 1)\n",
    "    return combined\n",
    "\n",
    "def calculate_input_dim(z_dim, mnist_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate the input dimensions for the generator and discriminator networks.\n",
    "\n",
    "    Args:\n",
    "        z_dim (int): Dimension of the random noise vector (latent space).\n",
    "        mnist_shape (tuple): Shape of the MNIST images, e.g., (1, 28, 28).\n",
    "        n_classes (int): Number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the generator input dimension and discriminator image channel.\n",
    "\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\"\"\"\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "\n",
    "    # mnist_shape[0] is 1 as its grayscale images\n",
    "    discriminator_image_channel = mnist_shape[0] + n_classes\n",
    "\n",
    "    return generator_input_dim, discriminator_image_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a25d097b-70f2-4b4b-9e9a-063808cc4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from utils import *\n",
    "\n",
    "####################################################\n",
    "def test_weights_init():\n",
    "    # Create a sample model with Conv2d and BatchNorm2d layers\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ConvTranspose2d(16, 3, kernel_size=3),\n",
    "        nn.BatchNorm2d(3)\n",
    "    )\n",
    "\n",
    "    # Initialize the model weights\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # Check the weights of Conv2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "\n",
    "    # Check the weights of BatchNorm2d layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            assert torch.allclose(module.weight.mean(), torch.tensor(0.0), atol=0.02)\n",
    "            assert torch.allclose(module.weight.std(), torch.tensor(0.02), atol=0.02)\n",
    "            assert torch.allclose(module.bias, torch.tensor(0.0))\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_weights_init()\n",
    "\n",
    "####################################################\n",
    "def test_concat_vectors():\n",
    "    # Create sample input tensors\n",
    "    x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    y = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "    # Perform concatenation\n",
    "    combined = concat_vectors(x, y)\n",
    "\n",
    "    # Check the output type and shape\n",
    "    assert isinstance(combined, torch.Tensor)\n",
    "    assert combined.shape == (2, 6)  # Expected shape after concatenation\n",
    "\n",
    "    # Check the values in the concatenated tensor\n",
    "    expected_combined = torch.tensor([[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]])\n",
    "    assert torch.allclose(combined, expected_combined)\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_concat_vectors()\n",
    "\n",
    "####################################################\n",
    "def test_calculate_input_dim():\n",
    "    # Set up sample inputs\n",
    "    z_dim = 100\n",
    "    mnist_shape = (1, 28, 28)\n",
    "    n_classes = 10\n",
    "\n",
    "    # Calculate input dimensions\n",
    "    generator_input_dim, discriminator_image_channel = calculate_input_dim(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "    # Check the output types and values\n",
    "    assert isinstance(generator_input_dim, int)\n",
    "    assert generator_input_dim == z_dim + n_classes\n",
    "\n",
    "    assert isinstance(discriminator_image_channel, int)\n",
    "    assert discriminator_image_channel == mnist_shape[0] + n_classes\n",
    "\n",
    "    print(\"Unit test passed!\")\n",
    "\n",
    "# Run the unit test\n",
    "# test_calculate_input_dim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e8c7fc96-cac3-443b-8b2f-6fe78895b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#from conditional_gan import *\n",
    "#from utils import *\n",
    "\n",
    "mnist_shape = (1, 340, 340)\n",
    "n_classes = 768\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 1\n",
    "z_dim = 2001\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = \"cuda\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7a08800b-adcd-486f-b7e6-31b3c4361bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataloader = DataLoader(\n",
    "#    MNIST(\n",
    "#        \"/home/dennis00/scRNA_GAN/MNIST\", download=True, transform=transform\n",
    "#    ),\n",
    "#    batch_size=batch_size,\n",
    "#    shuffle=True,\n",
    "#)\n",
    "\n",
    "generator_input_dim, discriminator_image_channel = calculate_input_dim(\n",
    "    z_dim, mnist_shape, n_classes\n",
    ")\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "disc = Discriminator(image_channel=discriminator_image_channel).to(device)\n",
    "\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "gen  = gen.apply(weights_init)\n",
    "\n",
    "disc = disc.apply(weights_init)\n",
    "\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e91a5b19-5641-4913-915b-4bfefe3a12f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(2769, 2048, kernel_size=(5, 5), stride=(5, 5))\n",
       "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f69795d4-58ef-440e-847b-53b8071f5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, cell_embeddings, smiles_embeddings, images):\n",
    "        self.cell_embeddings = torch.tensor(cell_embeddings, dtype=torch.float32)\n",
    "        self.smiles_embeddings = torch.tensor(smiles_embeddings, dtype=torch.float32)\n",
    "        self.images = torch.tensor(images, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cell_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.cell_embeddings[idx], \n",
    "            self.smiles_embeddings[idx], \n",
    "            self.images[idx]\n",
    "        )\n",
    "\n",
    "# Example usage for train set\n",
    "train_dataset = MultiModalDataset(train_cell_embeddings, train_smiles_embeddings, train_images)\n",
    "\n",
    "# Same for validation and test sets\n",
    "val_dataset = MultiModalDataset(valid_cell_embeddings, valid_smiles_embeddings, valid_images)\n",
    "test_dataset = MultiModalDataset(test_cell_embeddings, test_smiles_embeddings, test_images)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9cdbb2ef-10c0-4368-be7c-218d34c7d372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb311ee48f824e5ba5f366d4fbade0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move data to GPU if needed\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcur_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for cell, smile, real in tqdm(train_loader):\n",
    "        # Move data to GPU if needed\n",
    "        cell, smile, real = cell.to(device), smile.to(device), real.to(device)\n",
    "\n",
    "        cur_batch_size = len(cell)\n",
    "        \n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        one_hot_labels = smile\n",
    "        \n",
    "        fake_noise = create_noise_vector(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        noise_and_labels = concat_vectors(fake_noise, one_hot_labels)\n",
    "\n",
    "        fake = gen(noise_and_labels)\n",
    "\n",
    "        # Make sure that enough images were generated\n",
    "        assert len(fake) == len(real)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "78a41a7f-f1ac-4a38-82ce-9932b649632e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 340, 340])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4f26fc37-6b3a-451f-b161-7570ceded4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2769])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_and_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2c1689aa-7c7c-42ca-8399-0db9c1d084eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 340, 340])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5fbc6ef8-788b-493b-b37b-81391c8de738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4593, -0.2537,  0.0481,  ..., -0.0569, -0.2366,  0.7164],\n",
       "        [ 2.1339, -0.1549,  0.3125,  ..., -0.0910, -0.1731,  0.6211],\n",
       "        [ 2.5093,  0.9491,  1.0351,  ..., -0.1222, -0.4565,  0.9770],\n",
       "        ...,\n",
       "        [ 2.1316,  0.1801, -0.2926,  ..., -0.7508, -0.4878,  1.0630],\n",
       "        [ 1.0609,  0.1547, -0.3923,  ..., -0.3137, -0.0152,  0.7931],\n",
       "        [ 2.2265,  0.2990,  0.4958,  ...,  0.5109,  0.1003,  0.2555]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "859cc2d0-e5cd-44c9-bc2c-8f4eb86d76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_one_hot_labels = one_hot_labels[:, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b1251f0a-ead6-414e-97a8-603ce2136c14",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 42.33 GiB. GPU 0 has a total capacity of 31.74 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Process 432941 has 610.00 MiB memory in use. Of the allocated memory 19.02 GiB is allocated by PyTorch, and 10.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_one_hot_labels \u001b[38;5;241m=\u001b[39m \u001b[43mimage_one_hot_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m340\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m340\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 42.33 GiB. GPU 0 has a total capacity of 31.74 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Process 432941 has 610.00 MiB memory in use. Of the allocated memory 19.02 GiB is allocated by PyTorch, and 10.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "            1, 1, 340, 340\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7eab1955-73c4-4bd5-b1e8-98ca2d3e00eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 768, 1, 1])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc15544-904b-4094-bff4-a63c51316f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataloader = DataLoader(\n",
    "#    MNIST(\n",
    "#        \"/home/dennis00/scRNA_GAN/MNIST\", download=True, transform=transform\n",
    "#    ),\n",
    "#    batch_size=batch_size,\n",
    "#    shuffle=True,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "be50f973-b582-4cc9-b59c-7e86efe5c83c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m z_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Dataloader returns the batches and the labels\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m real, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdataloader\u001b[49m):\n\u001b[1;32m      5\u001b[0m         cur_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(real)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Flatten the batch of real images from the dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "z_dim = 64\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        # create one hot encoded vectors from labels and n_classes\n",
    "        one_hot_labels = ohe_vector_from_labels(labels.to(device), n_classes)\n",
    "        print(\"one_hot_labels \", one_hot_labels.size())  # => torch.Size([128, 10])\n",
    "\n",
    "        \"\"\" The above ([128, 10]) need to be converted to ([128, 10, 28, 28])\n",
    "\n",
    "        Because, Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "         1. All tensors need to have the same number of dimensions and\n",
    "         2. All dimensions except the one that they are concatenated on, need to have the same size.\n",
    "\n",
    "        To do that, first I am adding extra dimension with 'None'\n",
    "        the easiest way to add extra dimensions to an array is by using the keyword None,\n",
    "        when indexing at the position to add the extra dimension.\n",
    "        Note, in below with keyword None, I am only adding extra dummy empty dimension\n",
    "\n",
    "        a = torch.rand(1, 2)\n",
    "        ic(a) # => tensor([[0.1749, 0.6387]])\n",
    "        ic(a[None, :]) # => tensor([[[0.1749, 0.6387]]])\n",
    "\n",
    "        a = torch.rand([1,2,3,4])\n",
    "        ic(a.shape) # => torch.Size([1, 2, 3, 4])\n",
    "        ic(a[None, :].shape) # => torch.Size([1, 1, 2, 3, 4])\n",
    "        \"\"\"\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 1, 1])\n",
    "\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "            1, 1, mnist_shape[1], mnist_shape[2]\n",
    "        )\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 28, 28])\n",
    "\n",
    "        #########################\n",
    "        #  Train Discriminator\n",
    "        #########################\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size\n",
    "        fake_noise = create_noise_vector(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "        # Now we can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "\n",
    "        noise_and_labels = concat_vectors(fake_noise, one_hot_labels)\n",
    "        fake = gen(noise_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ada177-7add-49c7-aebd-c74c531442ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        # create one hot encoded vectors from labels and n_classes\n",
    "        one_hot_labels = ohe_vector_from_labels(labels.to(device), n_classes)\n",
    "        print(\"one_hot_labels \", one_hot_labels.size())  # => torch.Size([128, 10])\n",
    "\n",
    "        \"\"\" The above ([128, 10]) need to be converted to ([128, 10, 28, 28])\n",
    "\n",
    "        Because, Concatenation of Multiple Tensor with `torch.cat()` - RULE - To concatenate WITH torch.cat(), where the list of tensors are concatenated across the specified dimensions, requires 2 conditions to be satisfied\n",
    "\n",
    "         1. All tensors need to have the same number of dimensions and\n",
    "         2. All dimensions except the one that they are concatenated on, need to have the same size.\n",
    "\n",
    "        To do that, first I am adding extra dimension with 'None'\n",
    "        the easiest way to add extra dimensions to an array is by using the keyword None,\n",
    "        when indexing at the position to add the extra dimension.\n",
    "        Note, in below with keyword None, I am only adding extra dummy empty dimension\n",
    "\n",
    "        a = torch.rand(1, 2)\n",
    "        ic(a) # => tensor([[0.1749, 0.6387]])\n",
    "        ic(a[None, :]) # => tensor([[[0.1749, 0.6387]]])\n",
    "\n",
    "        a = torch.rand([1,2,3,4])\n",
    "        ic(a.shape) # => torch.Size([1, 2, 3, 4])\n",
    "        ic(a[None, :].shape) # => torch.Size([1, 1, 2, 3, 4])\n",
    "        \"\"\"\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 1, 1])\n",
    "\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "            1, 1, mnist_shape[1], mnist_shape[2]\n",
    "        )\n",
    "        print(\n",
    "            \"image_one_hot_labels.size \", image_one_hot_labels.size()\n",
    "        )  # => torch.Size([128, 10, 28, 28])\n",
    "\n",
    "        #########################\n",
    "        #  Train Discriminator\n",
    "        #########################\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size\n",
    "        fake_noise = create_noise_vector(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "        # Now we can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "\n",
    "        noise_and_labels = concat_vectors(fake_noise, one_hot_labels)\n",
    "        fake = gen(noise_and_labels)\n",
    "\n",
    "        # Make sure that enough images were generated\n",
    "        assert len(fake) == len(real)\n",
    "\n",
    "        # Now we can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels,\n",
    "        #              remember to detach the generator (.detach()) so we do not backpropagate\n",
    "        #              through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "\n",
    "        # Combine the fake images with image_one_hot_labels\n",
    "        fake_image_and_labels = concat_vectors(fake, image_one_hot_labels)\n",
    "\n",
    "        # Combine the real images with image_one_hot_labels\n",
    "        real_image_and_labels = concat_vectors(real, image_one_hot_labels)\n",
    "\n",
    "        # Get the discriminator's prediction on the reals and fakes\n",
    "        disc_fake_pred = disc(fake_image_and_labels.detach())\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "\n",
    "        # Make sure that enough predictions were made\n",
    "        assert len(disc_real_pred) == len(real)\n",
    "        # Make sure that the inputs are different\n",
    "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
    "\n",
    "        # Calculate Discriminator Loss on fakes and reals\n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "\n",
    "        # Get average Discriminator Loss\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step()\n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "        #########################\n",
    "        #  Train Generators\n",
    "        #########################\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        fake_image_and_labels = concat_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if we didn't concatenate wer labels to wer image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "\n",
    "        \"\"\" Now calculate Generator Loss and note that, here, unlike the disc_loss, with\n",
    "        disc_fake_pred, I am passing a vector containing its elements as 1 with torch.ones_like\n",
    "        Because, Generator wants to fool the Discriminator by telling it that all these fake images are actually real, i.e. with value of 1\n",
    "        \"\"\"\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "\n",
    "        ##################################\n",
    "        #  Log Progress and Visualization\n",
    "        #  for each display_step = 50\n",
    "        ##################################\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            # Calculate Generator Mean loss for the latest display_steps (i.e. latest 50 steps)\n",
    "            # list[-x:]   # last x items in the array\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "\n",
    "            # Calculate Discriminator Mean loss for the latest display_steps (i.e. latest 50 steps)\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            print(\n",
    "                f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\"\n",
    "            )\n",
    "\n",
    "            # Plot both the real images and fake generated images\n",
    "            plot_images_from_tensor(fake)\n",
    "            plot_images_from_tensor(real)\n",
    "\n",
    "            step_bins = 20\n",
    "            x_axis = sorted(\n",
    "                [i * step_bins for i in range(len(generator_losses) // step_bins)]\n",
    "                * step_bins\n",
    "            )\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(generator_losses[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Generator Loss\",\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(discriminator_losses[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Discriminator Loss\",\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\"Let Long Training Continue\")\n",
    "        cur_step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
